# Finetuned_LLaMA3.2-11B-Vison-Model_and_Inference_vLLM


# ðŸš€ Finetuned LLaMA 3.2-11B Vision Model and Inference with vLLM

This repository contains the **fine-tuned LLaMA 3.2-11B Vision Model** for **radiology image analysis**, along with **efficient inference using vLLM**. The model was fine-tuned using **LoRA (Low-Rank Adaptation)** with **Unsloth**, optimized with **4-bit quantization (bitsandbytes)**, and deployed for inference using **Chainlit**.

---

## ðŸ“– **Project Overview**
- **Model**: `LLaMA 3.2-11B Vision`
- **Fine-Tuning Framework**: `Unsloth`
- **Inference Engine**: `vLLM`
- **Dataset**: `unsloth/Radiology_mini`
- **Training Method**: `LoRA` (Parameter Efficient Fine-Tuning)
- **Quantization**: `4-bit`
- **Deployment**: `Chainlit` interactive chatbot

---

## ðŸ“‚ **Repository Structure**
```plaintext
ðŸ“¦ Finetuned_LLaMA3.2-11B-Vision-Model_and_Inference_vLLM
â”œâ”€â”€ chat.py                    # Chainlit-powered chatbot for image inference
â”œâ”€â”€ Fine_Tuned_LLaMA3.2_Model.ipynb  # Jupyter Notebook for model fine-tuning
â”œâ”€â”€ requirements.txt            # Required dependencies
â”œâ”€â”€ chainlit.md                 # Documentation for running Chainlit
â”œâ”€â”€ vllm/                       # Folder containing inference scripts
â”œâ”€â”€ __pycache__/                # Python cache files
â””â”€â”€ README.md                   # Project documentation (this file)


